* Introduction + Programming Paradigms in General

- Programming paradigms
 (pronounciation: ˈpærədaɪm (us?)) 
- DAT121

** The teaching team
*** JP Bernardy
Course responsible & Lecturer
Room: 6126
E-mail: bernardy@chalmers.se

CV:
1996-2000: Master in CS (Free University of Brussels)
2000-2007: Software engineer at various places
2007-2011: PhD (Chalmers)

*** Ramona Enache
Assistant
*** Michal Pałka
Assistant

** Schedule & Organisation
*** Formal requirements:
- Pass the exam
- Participate in the exercises correction sessions (bonus points)
- Do the exercises
- Attend the lectures
- Prepare lectures by reading lecture notes
*** Lectures
Please interrupt me during lectures!
*** Exercises
Time:
*** Office hours
JP:
Ramona:
Michal:
*** Course evaluation
3-4 Volunteers needed
- Discuss the course with other students, represent their views to the teachers.
- Take part in 4 meetings
**** TODO (when?)
- Cremona voucher for 200SEK?
- Volunteer today! 
*** Reading material

** What is a "programming paradigm"?

*** Definition

http://www.merriam-webster.com/dictionary/paradigm

"A philosophical and theoretical framework of a scientific school or
discipline within which theories, laws, and generalizations and the
experiments performed in support of them are formulated; broadly: a
philosophical or theoretical framework of any kind"

see also: http://en.wikipedia.org/wiki/Programming_paradigm

*** "Way of organising thought"


      Paradigms

         ^
         |

    Design Patterns

         ^
         |

      Programs

(I cannot teach you so many ways to organise your brain in 7 weeks!
But fear not... You have other courses :) )

*** "Mental model of the computer"

- von Neumann model
- Rewriting engine
- Mathematical functions


*** Languages
**** (Do not reveal:) Discussion: What languages do you know? 

Regexp / Excell formulas / sql queries / Haskell / C / Asm / ...

--> clouds / recognise paradigms / discussions

- Paradigms build on top of features
- Languages implement features

http://www.info.ucl.ac.be/~pvr/paradigmsDIAGRAMeng108.pdf

*** Features
- Structured data / Records
- Naming and abstraction (2nd order, etc).
- Memory (cell) / State
- Processes
- Communication channels
- Recursion
- Search


*** The importance of knowing multiple paradigms
**** Ability to think "big thoughts"
- Anecdote: MULTICS
- Further reading: "Language as thought shaper" http://soft.vub.ac.be/~tvcutsem/whypls.html


Language as thought shaper

To quote Alan Perlis: "a language that doesn't affect the way you think about programming, is not worth knowing."

The goal of a thought shaper language is to change the way a programmer thinks about structuring his or her program. The basic building blocks provided by a programming language, as well as the ways in which they can (or cannot) be combined, will tend to lead programmers down a "path of least resistance", for some unit of resistance. For example, an imperative programming style is definitely the path of least resistance in C. It's possible to write functional C programs, but as C does not make it the path of least resistance, most C programs will not be functional.

Functional programming languages, by the way, are a good example of thought shaper languages. By taking away assignment from the programmer's basic toolbox, the language really forces programmers coming from an imperative language to change their coding habits. I'm not just thinking of purely functional languages like Haskell. Languages like ML and Clojure make functional programming the path of least resistance, yet they don't entirely abolish side-effects. Instead, by merely de-emphasizing them, a program written in these languages can be characterized as a sea of immutability with islands of mutability, as opposed to a sea of mutability with islands of immutability. This subtle shift often makes it vastly easier to reason about the program.

Erlang's concurrency model based on isolated processes communicating by messages is another example of a language design that leads to radically different program structure, when compared to mainstream multithreading models. Dijkstra's "GOTO considered harmful" and Hoare's Communicating Sequential Processes are pioneering examples of the use of language design to reshape our thoughts on programming. In a more recent effort, Fortress wants to steer us towards writing parallel(izable) programs by default.

Expanding the analogy with natural languages, languages as thought shapers are not about changing the vocabulary or the grammar, but primarily about changing the concepts that we talk about. Erlang inherits most of its syntax from Prolog, but Erlang's concepts (processes, messages) are vastly different from Prolog's (unification, facts and rules, backtracking). As a programing language researcher, I really am convinced that language shapes thought.

**** Altenative paradigms in the industry:
- "Excell is the most used programming language"
- SQL is mostly functional (relational)
- F# officially supported by MicroSoft
- Exponential growth of Erlang / Haskell

**** Fun reading on the importance of using the right language:
http://tauday.com/

** Outline of the course
*** Brief exposition of each paradigm
- Functional (requirement: "introduction to functional programming" TDA555)
- Imperative (not a requirement: "machine-oriented programming" EDA480)
- Concurrent (not a requirement: "concurrent programming" TDA381)
- Object oriented (requirement: "Object oriented programming" DAT042)
- Logic (not a requirement: ?)
*** (Some) Transformations between paradigms
*** Learning outcomes
**** Awareness of multiple paradigms
First question of the design phase: "How should I think about this
problem?"
**** Recognise "encoded" thoughts:
***** what is the natural paradigm
***** decode them
**** Encode thoughts expressed in a paradigm in another one

**** The exam questions will be similar to exercises
Note in particular that exercises are integral part of the course material.

* Imperative programming

"von neumann" model of the computer:

- Memory cells
- Program (assignments, arithmetic, logic, (conditional) jumps)

** Example

*** Gotos

   -- Assume A : list of sortable items

   begin:
        swapped = false
        i := 1;
   loop:
        if A[i-1] <= A[i] goto no_swap
        swap( A[i-1], A[i] )
        swapped = true
   no_swap:
        i := i+1
        if i < n then goto loop
        if swapped goto begin

*** Loops & Ifs

   -- Assume A : list of sortable items

      while swapped
        swapped = false
        for each i in 1 to length(A) - 1 inclusive do:
          if A[i-1] > A[i] then
            swap( A[i-1], A[i] )
            swapped = true
          end if
        end for

*** Wrapping in a procedures for good measure

    procedure bubbleSort( A : list of sortable items )
      do
        swapped = false
        for each i in 1 to length(A) - 1 inclusive do:
          if A[i-1] > A[i] then
            swap( A[i-1], A[i] )
            swapped = true
          end if
        end for
      while swapped
    end procedure

** Discussion: When are gotos appropriate?
extra reading: "goto statement considered harmful", E. G. Dijkstra
http://portal.acm.org/citation.cfm?id=362947


** Transformation: Loops -> Gotos

*** Source: 
while cond do
  body

*** Target
test:
  p := cond
  if p goto done
  body
  goto test
done:

*** Exercise: translate the following

do
   body
until cond

** Transformation: If .. then .. else -> Gotos
*** Source
if cond then
  part1
else
  part2

*** Target
  p := not(cond)
  if p then goto label2
  part1
  goto done
label2:
  part2
done:

*** Exercise: switch/case

** Reverse transformation?

No general form! (You must be creative)

** Transformation: inlining procedures

*** Source
procedure g(x,y)
  x := x + y

procedure f(x,y)
  g(x,y)
  x := x + 1
  g(y,x)

f(a,b)

*** Intermediate

procedure f(x,y)
  x := x + y
  x := x + 1
  y := y + x


f(a,b)

*** Final

a := a + b
a := a + 1
b := b + x


*** Trivia: What happens when the original program is recursive?

** Transformation: Procedures -> Gotos

--------------------------------------
function sqrt(x : Float) : Float
  result := x / 2 
  while ...
    -- Newton approx to refine the result
  return result;

-- the calls:
sqrt(12345)
...
...
sqrt(6789);

------------------------------------


----------------------------------
sqrt:
-- argument in global variable 'sqrtArgument'
sqrtResult := sqrtArgument / 2;
-- And then newton algorithm 
...
...
-- at this point, sqrtResult contains the result.
goto sqrtCaller;


sqrtArgument := 12345;
sqrtCaller := out1;
goto sqrt;
out1:
...
...
sqrtArgument := 6789;
sqrtCaller := out1;
goto sqrt;
out1:
---------------------------------------

*** Trivia: What happens when the original program is recursive?

- Loop (dynamic)
- variables: a mess...
** Transformation: Explicit stack
*** 1st example: factorial.
Translation of a recursive call:
 - push local variables on a stack
 - goto
 - pop local variables

-----------------------------------
function fact (n:Int)
  if n = 0 then
    return 1
  else
    return n * fact(n-1)
-----------------------------------

Straightforward application of rules:

---------------------------------------
-- Call to 'fact'
caller := out;
n := 12;
goto fact;
out:

...
...

-- Definition of 'fact':
fact:
if n = 0 then
  result := 1;
  goto caller;
else 
  push(n,caller);
  caller := continue;
  n := n-1;
  goto fact;
  continue:
  pop(n,caller);
  result := n * result; -- result is the result of the recursive call.
  goto caller;
------------------------------------------------

*** 2nd example: factorial (alternative algorithm)

Other example:

--------------------------------
function fact (n:Int,acc:Int)
  if n = 0 then
    return acc
  else
    return fact(n-1,n * acc)
--------------------------------


What is the algorithm used?


--------------------------------
fact: -- n,acc,caller are defined here.
if n = 0 then
  result := acc;
  goto caller;
else
  push (n,acc,caller)
  acc := acc * n;
  n := n-1;
  caller := continue;
  goto fact;  
  continue:
  pop (n,acc,caller)
  result := result; -- just forward the result of the recursive call.
  goto caller;
--------------------------------


But:
 - The local variables are saved for nothing: they are not used after they are popped!
 - The result := result statement is useless.

Hence we obtain:


--------------------------------
fact: -- n,acc,caller are defined here.
if n = 0 then
  result := acc;
  goto caller;
else
  push (caller)
  acc := acc * n;
  n := n-1;
  caller := continue;
  goto fact;  
  continue:
  pop (caller)
  goto caller;
--------------------------------

What is the effect of the following?

  push (caller)
  caller := continue
  goto fact

It fact, it is the same as 

  goto fact

Indeed, after returning to "continue", the caller will just be popped
from the stack; and we'll jump to it.  This would also be done by the
normal "goto caller" return statement if we had not overwritten the
caller with continue.


Hence, the stack can be removed altogether! This is called /tail-call optimisation/. Why?


We get:
-----------------
acc := 1;
caller := out;
goto fact
out:

fact:
if n = 0 then
  result := acc;
  goto caller;
else
  acc := n * acc; -- note the order of assignments
  n := n-1;
  goto fact:
-----------------

Finally we can reconstruct a loop:


-------------------------
acc := 1;
while n /= 0 do
  acc := n * acc;
  n := n-1;
result := acc;
-------------------------



Exercise:
- Derecursify tree traversal
- Do you really need a stack? (hint: you can update the tree as you go)

** Passing by reference 

*** Reminder: References (aka. pointers)

 x : Integer

 addressOf(x) : ReferenceTo Integer
   ≃ where in the memory is the variable x

 variableAt(x : ReferenceTo Integer) : Integer


**** Trivia: whats the meaning of addressOf(addressOf(x))?
 -> none! because addressOf(x) is just a value, there is no location for it in the memory.
**** Exercise: write the above in C syntax


*** Example
**** Supposing the language supports by. ref.
increment(by ref. x : Int)
  x := x + 1


...


increment(y)

**** No by ref, but pointers:


increment(x : ReferenceTo Int)
  variableAt(x) := variableAt(x) + 1

...

increment(addressOf(y))

*** Uses of passing by reference (?)

- "expressive power" : you can factor out parts of the computation that update any (sub-part of) the state

- save time : no need to copy around things

*** Exercise: Does Java use call by reference? 
  Show example(s) that says yes/no
  
* Interlude: Garbage Collection
aka. Automatic memory management
- Allows for much easier OOP
- Practically impossible to do FP/Logic without it
* Object-oriented programming

** Coupling data and related code
*** Toy example: Date

class Date

  field
    year : Integer
    month : Integer
    day : Integer


  method ShiftByDays(days : Integer);

  constructor ymd(y,m,d : Integer)
  constructor today -- -- query OS for current date


-- Example use:
appointment = today;
appointment.shiftByDays(7);

**** Note: Objects are, almost invariably, passed by reference.

**** Tranlated into plain records + procedures

record Date
  Year : Integer
  Month : Integer
  Day : Integer
  

function today : Date;

procedure ShiftByDays(this : Date by reference; days : Integer);
-- Why is "by reference" important?
  

-- Example use:
appointment = today;
shiftByDays(appointment,7);

** Encapsulation 

mechanisms to make the fields private

*** Paradigm Shift: Abstract Data Type (ADT) 
 - Example: "stack", "priority queue", ... from your data structures course
 - Every piece data type comes with a specification
 - ... maybe in the form of _unit tests_
 - Notion of data-invariant
 - Advantage: it's easy to change representation of data

 - Note: not every piece of data fits the ADT model. 
   Example: "Person" record.
 - Dogma: never any direct field access (cf. "set" and "get")


** Inheritance

*** Toy example:

class Animal
  method Pet
     print "Undefined"

class Dog inherits Canis
  method Pet
     print "Shake tail"

class Cat inherits Canis 
  method Pet
     print "Mew"


procedure Test(c : Canis)
  c.Feed

Test(new Dog);
Test(new Cat);

*** Translated to records:

record Animal
  field
    Pet : function pointer;


record Dog 
  field
    Pet : function pointer;

procedure petDog(this : Dog);
  print "Shake tail"  -- (1)


function createDog : Dog
  return new Dog(pet = petDog);  
    

record Cat
  field 
     Pet : function pointer;

procedure petCat(this : Cat);
  print "Shake tail"


function createCat : Cat
  return new Cat(pet = petCat);  


procedure Test(c : Animal by reference)
  c.Pet; -- question: explain this line


Test(cast<Animal> createDog); -- why is the cast valid?
Test(cast<Animal> createCat);

*** Exercise: add a StrayCat subclass which: 
- scratches instead of meowing;
- counts of the number of wounds inflicted.

*** Extension: function tables

- Is the 'pet' function pointer ever modified?
- How can we save space if there are many methods per class? 


*** Paradigm Shift

  - Multiple "cases" can be implemented by inheriting a common class
  - Dogma: no "if".
  - Specific behaviour is implemented in derived methods
  
  - Open question: multiple dispatch!

** Reading/Exercise: Javascript prototypes
http://en.wikipedia.org/wiki/ECMAScript_syntax#Objects

** TODO multiple-inheritance & interfaces
** Forward reference: objects are poor man's _closures_

* Functional programming
** Reading (as necessary): "Learn you a Haskell, for great good!"
http://learnyouahaskell.com/

** A bit of syntax

*** Function definitions

minimum (x,y) = if x < y 
                  then x
                  else y

*** (λ) abstractions / local functions

In the literature:

minimum = λ(x,y). if x < y 
                      then x
                      else y


In Haskell:

minimum = \(x,y) -> if x < y 
                      then x
                      else y



*** Application BINDS TO THE LEFT.

f x   ==  f(x)

f x y == (f x) y  ==  (f(x))(y)


** Types: a crash course

0 : Int
1 : Int
"hello" : String

successor : Int -> Int

factorial : ?

π : ?

sin : ?

** Algebraic Data
   
If A and B are data types, then...

what is  A + B ?

         similar to union in C (what is the difference?)

         A × B ?

         similar to records in C


Let's count the number of inhabitants of the type:


    #(A + B) = #A + #B
    #(A × B) = #A × #B

To "bootstrap" we also need types 0 (empty type, unit of +) and 1 (singleton, unit of ×)

*** Exercise (⋆⋆⋆): what is A → B, algebraically ?

*** Examples

Bool = 1 + 1

Giving a name to the cases:

Bool = (True : 1) + (False : 1)

In Haskell syntax:

data Bool = True | False

Lists can be defined as follows, using _recursion_:
List a = (Nil : a) + (Cons : a × List a)


Haskell syntax:

data List a = Nil a | Cons a (List a)


Lists as a

*** Exercise: define an algebraic type for binary trees

*** Transformation: Algebraic data type -> inheritance

** Higher-order functions

*** Example: fold/reduce

-- sum the elements in a list
sum Nil          = 0
sum (Cons x xs)  = x + sum xs


-- multiply the elements in a list
product Nil         = 1
product (Cons x xs) = 1 * product xs


ABSTRACT! ABSTRACT! ABSTRACT!

...

*** TODO Example: map

*** Exercise: write a function that does the dot-product of a vector; then Abstract.

What do you get?    

*** Reading: (1st part compulsory)
"Can Programming Be Liberated From the von Neumann Style?", John
Backus, 1977 Turing Award Lecture
http://www.thocp.net/biographies/papers/backus_turingaward_lecture.pdf

** Removing Higher-Order functions
*** TODO Transformation: Inlining higher-order functions

Example/Exercise: from "filter/map" to for loop...



inverse of abstraction

map : (a -> b) -> List a -> List b
map f xs = case xs of 
   [] ->  []
   (x:xs) -> f x : map f xs


multiply n xs = map (\x -> x * n) xs


replace 'f' by its value in the code of 'map':


multiply n xs = case xs of
    [] ->  []
    (x:xs) -> (\x -> x * n) x : recursiveCall f xs


β-reduce:

multiply n xs = case xs of
    [] ->  []
    (x:xs) -> x * n : recursiveCall f xs


Downside: 
- explosion of the code size
- maybe impossible! (eg. the code of map is not available -- map itself is abstract)


*** TODO Transformation: Defunctionalisation (explicit closures)

http://en.wikipedia.org/wiki/Closure_(computer_science)


map : (a -> b) -> List a -> List b
map f [] = []
map f (x:xs) = f x : map f xs


multiply n = map (\x -> x * n) 


map : Closure -> List a -> List b
map f [] = []
map f (x:xs) = apply f x : map f xs


multiply n = map (Multiply n)

apply (Multiply n) x = x * n

data Closure = Multiply Int | ...


** Transformation: Explicit State

Idea: pass around the "state of the world" explicitly

print : () -- in an imperative language, the state is implicit

print : State -> State × () -- after making the state explicit



Assuming the "state of the world" is only the contents of the output
file, then print does what?

Exercise: implement "safePrint" functionally...

procedure safePrint(line) : ErrorCode
  if outOfInk then
    return -1
  else
    print(line)

... given the imperative function

outOfInk : Bool

 1. What is the type of outOfInk in the functional representation ?
 2. What is the translation ?

*** Imperative syntax in Haskell

-- "IP a": type of imperative programs returning a value of type a.
type IP a = State -> State × a

Generic way to sequence two "IP a":

andThen : IP a -> IP b -> IP b
f `andThen` g = \s0 -> let (s1,a) = f s0
                           (s2,b) = g s1
                       in  (s2,b)


But what if the 2nd program uses the returned value of the 1st?
Then (in general) the 2nd program must depend on 'a':

andThen : IP a -> (a -> IP b) -> IP b
f `andThen` g = \s0 -> let (s1,a) = f s0
                           (s2,b) = g a s1
                       in  (s2,b)

If you _can_ define a function with the above type, then Haskell gives
you special syntax for imperative programming. If you give:

instance Monad IP where
  (>>=) = andThen
  return x = -- when x does not depend on the state:


Then the following is valid:


safePrint line = do
  condition <- outOfInk  
  if outOfInk 
    then return -1
    else do print line
            return 0
            
In fact, the meaning of "imperative" is given by that function -- andThen in our case:

safePrint line = 
  outOfInk `andThen` \condition ->
  if outOfInk 
    then return -1
    else print line `andThen` \() ->
         return 0

** Transformation: Currification


f : (A × B) → C
f = ...

g : A → (B → C)
g a = \b -> f (a,b)


h : (A × B) → C
h (a,b) = g a b

*** Note: try to read A → B as B^A
  then what is currification
**** Extra: can you implement other algebraic laws?

** Purity and its Consequences

Did you know that side effects...
 - are a common source of bugs?
 - make testing difficult?
 - make reasoning difficult?
 - make parallelizing diffcult?
 - cause cancer?

*** Referential transparency

    Mathematical function (sin)

     vs. 

    Function in (say) Java (getChar)

*** Testing is MUCH easier

       (no guesswork to know what a function depends on)

*** More optimisations possible (which ones?)
*** Easier concurrency (cf. Erlang)

    x = 0
    x = x+1 |in parallel with| x = x + 1
  
    Value of x ?     

*** Sharing is ALWAYS safe! (see in a moment)

*** Possible to use laziness  (see in a moment)

** Copying and sharing

Example: tree update

** Laziness

*** Question: How much memory is used by map?

- l : List Int
- length l = n
- How much is consumed by:

    map (+1) l

**** Same question, but assume that only the 1st element of the new list is used in the rest of the program

**** Same question, but assume 'l' is no longer used in the rest of the program.

-> Some say: "in Haskell, lists are a _control structure_".

** Paradigm shift:

- When writing a search function, the programmer can ALWAYS (and ONLY)
  return a list of ALL possible results.

*** Trivia: what is the most used lazy language?
Probably SQL!

*** Read: _Why functional programming matters_, J. Hughes.


** Transformation: explicit thunks



* Concurrent programming

Disclaimer: Concurrent programming ≠ Parallel programming

"The world is concurrent!"

** Process
file:Process.hs
** Channel
file:Channel.hs
** Transformation: variable-managing process
file:CSPVariable.hs
** Transformation: explicit continuations
A (trivial) server:
file:Server.hs
Same with explicit continuations:
file:ServerWithContinuations.hs
*** Exercise: make continuations explicit closures
* TODO Logic programming

** Unification
*** Metavariables
** Search

*** List of successes
*** Backtracking
manual search       <-->  constraints

http://stackoverflow.com/questions/2280021/logic-variables-support-for-net


Performance of inverted functions can be terrible.

* Overview of transformations

The following graph is an overview of all the transformations seen in
the course.

NOTE: You should know also how to "invert" a transformation!

#+begin_src dot :file some_filename.png :cmdline -Kdot -Tpng
digraph G {
   Imperative -> Machine [label=explicit stack\n(derecursification)]
   Object-Oriented -> Imperative [label=explicit dispatch]
   Functional -> Imperative [label=explicit closures\n(defunctionalization)]
   Functional -> Imperative [label=inline higher-order]
   Imperative -> Functional [label=explicit state]
   Functional -> Functional [label=explicit thunks]
   Imperative -> Concurrent [label=process managing state]
   Concurrent -> Functional [label=explicit continuations]   
   Concurrent -> Imperative [label=state in a process]
   Functional -> Logic [label=explicit result\n(embedding functions into relations)]
   Logic -> Functional [label=explicit list of successes]
}
#+end_src

* Where to go from here?
** Exam :)
** Explore the paradigms you like!
** Invent you own paradigm!
- ... that suits the way you think
- ... that suits your favourite application domain (AFP Course)
** A lot more to read ...
*** A poor man's concurrency monad (Claessen)
*** The essence of list comprehensions (Wadler)
*** Andre Pang's thesis
*** Introduction to programming with shift and reset
http://okmij.org/ftp/continuations/index.html#tutorial1
*** Transforming failure into a list of successes (Wadler)


